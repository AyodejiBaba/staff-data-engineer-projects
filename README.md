# Petabyte-Scale Data Pipelines

This repo demonstrates a full end-to-end petabyte-scale data platform:
- **Batch** jobs on Spark (Python)
- **Real-time** ingestion via Spark Structured Streaming (Scala) + Kafka
- **Cloud infra** provisioning in Terraform for Azure Databricks & GCP resources
- **CI/CD** with GitHub Actions
- **Data quality** checks with Great Expectations

## Quickstart
<!-- steps 1â€“8 here -->
## ðŸš€ Quickstart

1. **Clone**  
   ```bash
   git clone git@github.com:<you>/petabyte-scale-data-pipelines.git
   cd petabyte-scale-data-pipelines
